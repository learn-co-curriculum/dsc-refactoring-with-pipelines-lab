{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# Refactoring Your Code to Use Pipelines - Lab\n", "\n", "## Introduction\n", "\n", "In this lab, you will practice refactoring existing scikit-learn code into code that uses pipelines.\n", "\n", "## Objectives\n", "\n", "You will be able to:\n", "\n", "* Practice reading and interpreting existing scikit-learn preprocessing code\n", "* Think logically about how to organize steps with `Pipeline`, `ColumnTransformer`, and `FeatureUnion`\n", "* Refactor existing preprocessing code into a pipeline"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Ames Housing Data Preprocessing Steps\n", "\n", "In this lesson we will return to the Ames Housing dataset to perform some familiar preprocessing steps, then add an `ElasticNet` model as the estimator.\n", "\n", "#### 1. Drop Irrelevant Columns\n", "\n", "For the purposes of this lab, we will only be using a subset of all of the features present in the Ames Housing dataset. In this step you will drop all irrelevant columns.\n", "\n", "#### 2. Handle Missing Values\n", "\n", "Often for reasons outside of a data scientist's control, datasets are missing some values. In this step you will assess the presence of NaN values in our subset of data, and use `MissingIndicator` and `SimpleImputer` from the `sklearn.impute` submodule to handle any missing values.\n", "\n", "#### 3. Convert Categorical Features into Numbers\n", "\n", "A built-in assumption of the scikit-learn library is that all data being fed into a machine learning model is already in a numeric format, otherwise you will get a `ValueError` when you try to fit a model. In this step you will `OneHotEncoder`s to replace columns containing categories with \"dummy\" columns containing 0s and 1s.\n", "\n", "#### 4. Add Interaction Terms\n", "\n", "This step gets into the feature engineering part of preprocessing. Does our model improve as we add interaction terms? In this step you will use a `PolynomialFeatures` transformer to augment the existing features of the dataset.\n", "\n", "#### 5. Scale Data\n", "\n", "Because we are using a model with regularization, it's important to scale the data so that coefficients are not artificially penalized based on the units of the original feature. In this step you will use a `StandardScaler` to standardize the units of your data."]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Getting the Data\n", "\n", "The cell below loads the Ames Housing data into the relevant train and test data, split into features and target."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Run this cell without changes\n", "import pandas as pd\n", "from sklearn.model_selection import train_test_split\n", "\n", "# Read in the data and separate into X and y\n", "df = pd.read_csv(\"data/ames.csv\")\n", "y = df[\"SalePrice\"]\n", "X = df.drop(\"SalePrice\", axis=1)\n", "\n", "# Train-test split\n", "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Original Preprocessing Code\n", "\n", "The following code uses scikit-learn to complete all of the above steps, outside of the context of a pipeline. It is broken down into functions for improved readability.\n", "\n", "*Step 1: Drop Irrelevant Columns*"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Run this cell without changes\n", "def drop_irrelevant_columns(X):\n", "    relevant_columns = [\n", "        'LotFrontage',  # Linear feet of street connected to property\n", "        'LotArea',      # Lot size in square feet\n", "        'Street',       # Type of road access to property\n", "        'OverallQual',  # Rates the overall material and finish of the house\n", "        'OverallCond',  # Rates the overall condition of the house\n", "        'YearBuilt',    # Original construction date\n", "        'YearRemodAdd', # Remodel date (same as construction date if no remodeling or additions)\n", "        'GrLivArea',    # Above grade (ground) living area square feet\n", "        'FullBath',     # Full bathrooms above grade\n", "        'BedroomAbvGr', # Bedrooms above grade (does NOT include basement bedrooms)\n", "        'TotRmsAbvGrd', # Total rooms above grade (does not include bathrooms)\n", "        'Fireplaces',   # Number of fireplaces\n", "        'FireplaceQu',  # Fireplace quality\n", "        'MoSold',       # Month Sold (MM)\n", "        'YrSold'        # Year Sold (YYYY)\n", "    ]\n", "    return X.loc[:, relevant_columns]"]}, {"cell_type": "markdown", "metadata": {}, "source": ["*Step 2: Handle Missing Values*"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Run this cell without changes\n", "\n", "from sklearn.impute import MissingIndicator, SimpleImputer\n", "\n", "def handle_missing_values(X):\n", "    # Replace fireplace quality NaNs with \"N/A\"\n", "    X[\"FireplaceQu\"] = X[\"FireplaceQu\"].fillna(\"N/A\")\n", "    \n", "    # Missing indicator for lot frontage\n", "    frontage = X[[\"LotFrontage\"]]\n", "    missing_indicator = MissingIndicator()\n", "    frontage_missing = missing_indicator.fit_transform(frontage)\n", "    X[\"LotFrontage_Missing\"] = frontage_missing\n", "    \n", "    # Imputing missing values for lot frontage\n", "    imputer = SimpleImputer(strategy=\"median\")\n", "    frontage_imputed = imputer.fit_transform(frontage)\n", "    X[\"LotFrontage\"] = frontage_imputed\n", "    \n", "    return X, [missing_indicator, imputer]"]}, {"cell_type": "markdown", "metadata": {}, "source": ["*Step 3: Convert Categorical Features into Numbers*"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Run this cell without changes\n", "\n", "from sklearn.preprocessing import LabelBinarizer, OneHotEncoder\n", "\n", "def handle_categorical_data(X):\n", "    # Binarize street\n", "    street = X[\"Street\"]\n", "    binarizer_street = LabelBinarizer()\n", "    street_binarized = binarizer_street.fit_transform(street)\n", "    X[\"Street\"] = street_binarized\n", "    \n", "    # Binarize frontage missing\n", "    frontage_missing = X[\"LotFrontage_Missing\"]\n", "    binarizer_frontage_missing = LabelBinarizer()\n", "    frontage_missing_binarized = binarizer_frontage_missing.fit_transform(frontage_missing)\n", "    X[\"LotFrontage_Missing\"] = frontage_missing_binarized\n", "    \n", "    # One-hot encode fireplace quality\n", "    fireplace_quality = X[[\"FireplaceQu\"]]\n", "    ohe = OneHotEncoder(categories=\"auto\", sparse=False, handle_unknown=\"ignore\")\n", "    fireplace_quality_encoded = ohe.fit_transform(fireplace_quality)\n", "    fireplace_quality_encoded = pd.DataFrame(\n", "        fireplace_quality_encoded,\n", "        columns=ohe.categories_[0],\n", "        index=X.index\n", "    )\n", "    X.drop(\"FireplaceQu\", axis=1, inplace=True)\n", "    X = pd.concat([X, fireplace_quality_encoded], axis=1)\n", "    \n", "    return X, [binarizer_street, binarizer_frontage_missing, ohe]"]}, {"cell_type": "markdown", "metadata": {}, "source": ["*Step 4: Add Interaction Terms*"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Run this cell without changes\n", "\n", "from sklearn.preprocessing import PolynomialFeatures\n", "\n", "def add_interaction_terms(X):\n", "    poly_column_names = [\n", "        \"LotFrontage\",\n", "        \"LotArea\",\n", "        \"OverallQual\",\n", "        \"YearBuilt\",\n", "        \"GrLivArea\"\n", "    ]\n", "    poly_columns = X[poly_column_names]\n", "    \n", "    # Generate interaction terms\n", "    poly = PolynomialFeatures(interaction_only=True, include_bias=False)\n", "    poly_columns_expanded = poly.fit_transform(poly_columns)\n", "    poly_columns_expanded = pd.DataFrame(\n", "        poly_columns_expanded,\n", "        columns=poly.get_feature_names(poly_column_names),\n", "        index=X.index\n", "    )\n", "    \n", "    # Replace original columns with expanded columns\n", "    # including interaction terms\n", "    X.drop(poly_column_names, axis=1, inplace=True)\n", "    X = pd.concat([X, poly_columns_expanded], axis=1)\n", "    return X, [poly]\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["*Step 5: Scale Data*"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Run this cell without changes\n", "\n", "from sklearn.preprocessing import StandardScaler\n", "\n", "def scale(X):\n", "    scaler = StandardScaler()\n", "    X_scaled = scaler.fit_transform(X)\n", "    X = pd.DataFrame(\n", "        X_scaled,\n", "        columns=X.columns,\n", "        index=X.index\n", "    )\n", "    return X, [scaler]"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In the cell below, we execute all of the above steps on the training data:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Run this cell without changes\n", "\n", "from sklearn.linear_model import ElasticNet\n", "\n", "X_train = drop_irrelevant_columns(X_train)\n", "X_train, step_2_transformers = handle_missing_values(X_train)\n", "X_train, step_3_transformers = handle_categorical_data(X_train)\n", "X_train, step_4_transformers = add_interaction_terms(X_train)\n", "X_train, step_5_transformers = scale(X_train)\n", "\n", "model = ElasticNet(random_state=1)\n", "model.fit(X_train, y_train)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["(The transformers have all been returned by the functions, so theoretically we could use them to transform the test data appropriately, but for now we'll skip that step for time.)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Refactoring into a Pipeline\n", "\n", "Great, now let's refactor that into pipeline code! Some of the following code has been completed for you, whereas other code you will need to fill in.\n", "\n", "First we'll reset the values of our data:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Run this cell without changes\n", "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### 1. Drop Irrelevant Columns\n", "\n", "Previously, we just used pandas dataframe slicing to select the relevant elements. Now we'll need something a bit more complicated.\n", "\n", "When using `ColumnTransformer`, the default behavior is to drop irrelevant columns anyway. So what we really need now is to break down the full set of columns into:\n", "\n", "* Columns that should \"pass through\" without any changes made\n", "* Columns that require preprocessing\n", "* Columns we don't want\n", "\n", "Luckily we don't actually need a list of the third category, since they will be dropped by default.\n", "\n", "In the cell below, we create the necessary lists for you:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Run this cell without changes\n", "\n", "relevant_columns = [\n", "    'LotFrontage',  # Linear feet of street connected to property\n", "    'LotArea',      # Lot size in square feet\n", "    'Street',       # Type of road access to property\n", "    'OverallQual',  # Rates the overall material and finish of the house\n", "    'OverallCond',  # Rates the overall condition of the house\n", "    'YearBuilt',    # Original construction date\n", "    'YearRemodAdd', # Remodel date (same as construction date if no remodeling or additions)\n", "    'GrLivArea',    # Above grade (ground) living area square feet\n", "    'FullBath',     # Full bathrooms above grade\n", "    'BedroomAbvGr', # Bedrooms above grade (does NOT include basement bedrooms)\n", "    'TotRmsAbvGrd', # Total rooms above grade (does not include bathrooms)\n", "    'Fireplaces',   # Number of fireplaces\n", "    'FireplaceQu',  # Fireplace quality\n", "    'MoSold',       # Month Sold (MM)\n", "    'YrSold'        # Year Sold (YYYY)\n", "]\n", "\n", "poly_column_names = [\n", "    \"LotFrontage\",\n", "    \"LotArea\",\n", "    \"OverallQual\",\n", "    \"YearBuilt\",\n", "    \"GrLivArea\"\n", "]\n", "\n", "# Use set logic to combine lists without overlaps while maintaining order\n", "columns_needing_preprocessing = [\"FireplaceQu\", \"LotFrontage\", \"Street\"] \\\n", "    + list(set(poly_column_names) - set([\"LotFrontage\"]))\n", "passthrough_columns = list(set(relevant_columns) - set(columns_needing_preprocessing))\n", "\n", "print(\"Need preprocessing:\", columns_needing_preprocessing)\n", "print(\"Passthrough:\", passthrough_columns)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In the cell below, replace `None` to build a `ColumnTransformer` that keeps only the columns in `columns_needing_preprocessing` and `passthrough_columns`. We'll use an empty `FunctionTransformer` as a placeholder transformer for each. (In other words, there is no actual transformation happening, we are only using `ColumnTransformer` to select columns for now.)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Replace None with appropriate code\n", "\n", "from sklearn.compose import ColumnTransformer\n", "from sklearn.preprocessing import FunctionTransformer\n", "\n", "relevant_cols_transformer = ColumnTransformer(transformers=[\n", "    # Some columns will be used for preprocessing/feature engineering\n", "    (\"preprocess\", FunctionTransformer(), None), # <-- replace None\n", "    # Some columns just pass through\n", "    (\"passthrough\", FunctionTransformer(), None) # <-- replace None\n", "], remainder=\"drop\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Now, run this code to see if your `ColumnTransformer` was set up correctly:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Run this cell without changes\n", "\n", "from sklearn.pipeline import Pipeline\n", "\n", "pipe = Pipeline(steps=[\n", "    (\"relevant_cols\", relevant_cols_transformer)\n", "])\n", "\n", "pipe.fit_transform(X_train)\n", "\n", "# Transform X_train and create dataframe for readability\n", "X_train_transformed = pipe.fit_transform(X_train)\n", "pd.DataFrame(\n", "    X_train_transformed,\n", "    columns=columns_needing_preprocessing + passthrough_columns\n", ")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["(If you get the error message `ValueError: No valid specification of the columns. Only a scalar, list or slice of all integers or all strings, or boolean mask is allowed`, make sure you actually replaced all of the `None` values above.)\n", "\n", "If you're getting stuck here, look at the solution branch in order to move forward."]}, {"cell_type": "markdown", "metadata": {}, "source": ["Great! Now we have only the 15 relevant columns selected. They are in a different order, but the overall effect is the same as the `drop_irrelevant_columns` function above. The pipeline structure looks like this:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Run this cell without changes\n", "from sklearn import set_config\n", "set_config(display='diagram')\n", "pipe"]}, {"cell_type": "markdown", "metadata": {}, "source": ["You can click on the various elements (e.g. \"relevant_cols: ColumnTransformer\") to see more details."]}, {"cell_type": "markdown", "metadata": {}, "source": ["### 2. Handle Missing Values\n", "\n", "Same as before, we actually have two parts of handling missing values:\n", "\n", "* Imputing missing values for `FireplaceQu` and `LotFrontage`\n", "* Adding a missing indicator column for `LotFrontage`\n", "\n", "\n", "Let's start with imputing missing values.\n", "\n", "#### Imputing `FireplaceQu`\n", "\n", "The NaNs in `FireplaceQu` (fireplace quality) are not really \"missing\" data, they just reflect homes without fireplaces. Previously we simply used pandas to replace these values:\n", "\n", "```python\n", "X[\"FireplaceQu\"] = X[\"FireplaceQu\"].fillna(\"N/A\")\n", "```\n", "\n", "In a pipeline, we want to use a `SimpleImputer` to achieve the same thing. One of the available \"strategies\" of a `SimpleImputer` is \"constant\", meaning we fill every NaN with the same value.\n", "\n", "Let's nest this logic inside of a pipeline, because we know we'll also need to one-hot encode `FireplaceQu` eventually.\n", "\n", "In the cell below, replace `None` to specify the list of columns that this transformer should apply to:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Replace None with appropriate code\n", "\n", "# Pipeline for all FireplaceQu steps\n", "fireplace_qu_pipe = Pipeline(steps=[\n", "    (\"impute\", SimpleImputer(strategy=\"constant\", fill_value=\"N/A\"))\n", "])\n", "\n", "# ColumnTransformer for columns requiring preprocessing only\n", "preprocess_cols_transformer = ColumnTransformer(transformers=[\n", "    (\"fireplace_qu\", fireplace_qu_pipe, None) # <-- replace None\n", "], remainder=\"passthrough\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Now run this code to see if that `ColumnTransformer` is correct:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Run this cell without changes\n", "\n", "relevant_cols_transformer = ColumnTransformer(transformers=[\n", "    # Some columns will be used for preprocessing/feature engineering\n", "    (\"preprocess\", preprocess_cols_transformer, columns_needing_preprocessing),\n", "    # Some columns just pass through\n", "    (\"passthrough\", FunctionTransformer(), passthrough_columns)\n", "], remainder=\"drop\")\n", "\n", "pipe = Pipeline(steps=[\n", "    (\"relevant_cols\", relevant_cols_transformer)\n", "])\n", "\n", "pipe.fit_transform(X_train)\n", "\n", "# Transform X_train and create dataframe for readability\n", "X_train_transformed = pipe.fit_transform(X_train)\n", "pd.DataFrame(\n", "    X_train_transformed,\n", "    columns=columns_needing_preprocessing + passthrough_columns\n", ")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["(If you get `ValueError: 1D data passed to a transformer that expects 2D data. Try to specify the column selection as a list of one item instead of a scalar.`, make sure you specified a *list* of column names, not just the column name. It should be a list of length 1.)\n", "\n", "Now we can see \"N/A\" instead of \"NaN\" in those `FireplaceQu` records. We can also look at the structure of the pipeline, which is more complex now:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Run this cell without changes\n", "pipe"]}, {"cell_type": "markdown", "metadata": {}, "source": ["#### Imputing `LotFrontage`\n", "\n", "This is actually a bit simpler than the `FireplaceQu` imputation, since `LotFrontage` can be used for modeling as soon as it is imputed (rather than requiring additional conversion of categories to numbers). So, we don't need to create a pipeline for `LotFrontage`, we just need to add another transformer tuple to `preprocess_cols_transformer`.\n", "\n", "This time we left all three parts of the tuple as `None`. Those values need to be:\n", "\n", "* A string giving this step a name\n", "* A `SimpleImputer` with `strategy=\"median\"`\n", "* A list of relevant columns (just `LotFrontage` here)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Replace None with appropriate code\n", "\n", "preprocess_cols_transformer = ColumnTransformer(transformers=[\n", "    (\"fireplace_qu\", fireplace_qu_pipe, [\"FireplaceQu\"]),\n", "    (\n", "        None, # a string name for this transformer\n", "        None, # SimpleImputer with strategy=\"median\"\n", "        None  # List of relevant columns\n", "    )\n", "], remainder=\"passthrough\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Now that we've updated the `preprocess_cols_transformer`, check that the NaNs are gone from `LotFrontage`:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Run this cell without changes\n", "\n", "relevant_cols_transformer = ColumnTransformer(transformers=[\n", "    # Some columns will be used for preprocessing/feature engineering\n", "    (\"preprocess\", preprocess_cols_transformer, columns_needing_preprocessing),\n", "    # Some columns just pass through\n", "    (\"passthrough\", FunctionTransformer(), passthrough_columns)\n", "], remainder=\"drop\")\n", "\n", "pipe = Pipeline(steps=[\n", "    (\"relevant_cols\", relevant_cols_transformer)\n", "])\n", "\n", "pipe.fit_transform(X_train)\n", "\n", "# Transform X_train and create dataframe for readability\n", "X_train_transformed = pipe.fit_transform(X_train)\n", "pd.DataFrame(\n", "    X_train_transformed,\n", "    columns=columns_needing_preprocessing + passthrough_columns\n", ")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["#### Adding a Missing Indicator\n", "\n", "If you recall from the previous lesson, a `FeatureUnion` is useful when you want to combine engineered features with original (but preprocessed) features.\n", "\n", "In this case, we are treating a `MissingIndicator` as an engineered feature, which should appear as the last column in our `X` data regardless of whether there are actually any missing values in `LotFrontage`.\n", "\n", "First, let's refactor our entire pipeline so far, so that it uses a `FeatureUnion`:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Run this cell without changes\n", "\n", "from sklearn.pipeline import FeatureUnion\n", "\n", "### Original features ###\n", "\n", "# Preprocess fireplace quality\n", "fireplace_qu_pipe = Pipeline(steps=[\n", "    (\"impute\", SimpleImputer(strategy=\"constant\", fill_value=\"N/A\"))\n", "])\n", "\n", "# ColumnTransformer for columns requiring preprocessing\n", "preprocess_cols_transformer = ColumnTransformer(transformers=[\n", "    (\"fireplace_qu\", fireplace_qu_pipe, [\"FireplaceQu\"]),\n", "    (\"impute_frontage\", SimpleImputer(strategy=\"median\"), [\"LotFrontage\"])\n", "], remainder=\"passthrough\")\n", "\n", "# ColumnTransformer for all original features that we want to keep\n", "relevant_cols_transformer = ColumnTransformer(transformers=[\n", "    (\"preprocess\", preprocess_cols_transformer, columns_needing_preprocessing),\n", "    (\"passthrough\", FunctionTransformer(), passthrough_columns)\n", "], remainder=\"drop\")\n", "\n", "### Combine all features ###\n", "\n", "# Feature union (currently only contains original features)\n", "feature_union = FeatureUnion(transformer_list=[\n", "    (\"original_features\", relevant_cols_transformer)\n", "])\n", "\n", "# Pipeline (currently only contains feature union)\n", "pipe = Pipeline(steps=[\n", "    (\"all_features\", feature_union)\n", "])\n", "pipe.fit_transform(X_train)\n", "\n", "# Transform X_train and create dataframe for readability\n", "X_train_transformed = pipe.fit_transform(X_train)\n", "pd.DataFrame(\n", "    X_train_transformed,\n", "    columns=columns_needing_preprocessing + passthrough_columns\n", ")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Now we can add another item to the `FeatureUnion`!\n", "\n", "Specifically, we want a `MissingIndicator` that applies only to `LotFrontage`. So that means we need a new `ColumnTransformer` with a `MissingIndicator` inside of it.\n", "\n", "In the cell below, replace `None` to complete the new `ColumnTransformer`."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Replace None with appropriate code\n", "\n", "feature_eng = ColumnTransformer(transformers=[\n", "    (\n", "        None, # name for this transformer\n", "        None, # MissingIndicator with features=\"all\"\n", "        None  # list of relevant columns\n", "    )\n", "], remainder=\"drop\")"]}, {"cell_type": "code", "execution_count": 19, "metadata": {}, "outputs": [], "source": ["# Replace None with appropriate code\n", "\n", "feature_eng = ColumnTransformer(transformers=[\n", "    (\n", "        \"frontage_missing\",\n", "        MissingIndicator(features=\"all\"),\n", "        [\"LotFrontage\"]\n", "    )\n", "], remainder=\"drop\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Now run the following code to add this `ColumnTransformer` to the `FeatureUnion` and fit a new pipeline. If you scroll all the way to the right, you should see a new column, `LotFrontage_Missing`!"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Run this cell without changes\n", "\n", "# Feature union (currently only contains original features)\n", "feature_union = FeatureUnion(transformer_list=[\n", "    (\"original_features\", relevant_cols_transformer),\n", "    (\"engineered_features\", feature_eng)\n", "])\n", "\n", "# Pipeline (currently only contains feature union)\n", "pipe = Pipeline(steps=[\n", "    (\"all_features\", feature_union)\n", "])\n", "pipe.fit_transform(X_train)\n", "\n", "# Transform X_train and create dataframe for readability\n", "X_train_transformed = pipe.fit_transform(X_train)\n", "pd.DataFrame(\n", "    X_train_transformed,\n", "    columns=columns_needing_preprocessing + passthrough_columns + [\"LotFrontage_Missing\"]\n", ")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Now we should have a dataframe with 16 columns: our original 15 relevant columns (in various states of preprocessing completion) plus a new engineered column."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}], "metadata": {"kernelspec": {"display_name": "Python (learn-env)", "language": "python", "name": "learn-env"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.8.5"}}, "nbformat": 4, "nbformat_minor": 4}